{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Model ####\n",
    "MODELNUM = 2\n",
    "SCHEME = 'PrevailingWind_6Split'\n",
    "FEATURES = ['Lat', 'Lon', 'Alt', 'Temp', 'Precip', 'Year', 'JulianDay_Sin']\n",
    "\n",
    "### Import Libraries\n",
    "# Base Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import re\n",
    "import json\n",
    "# Tensorflow, scikit, kerasTuner\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, InputLayer\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import RootMeanSquaredError, MeanAbsoluteError\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import keras_tuner as kt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to import a dataset and transform headers for easier coding and convert Date column\n",
    "# Pseudocode:\n",
    "# 1. Import Dataset Save old Headers for later\n",
    "# 2. Transform headernames to rid of units\n",
    "# 3. Convert Date into Year and Julian Day\n",
    "# 4. Peform a Sine Transformation on JulianDay\n",
    "# 5. Return the dataset, and old headers\n",
    "def importData(fileName):\n",
    "    # Read in the correct file\n",
    "    dataset = pd.read_csv(f'../../Data/{fileName}.csv')\n",
    "    oldCols = list(dataset.columns)\n",
    "\n",
    "    # Remove any units (anything in parentheses)\n",
    "    codeCols = list(map(lambda x: re.sub(r'\\(([^()]*)\\)', '', x).strip(), oldCols))\n",
    "    dataset.columns = codeCols\n",
    "\n",
    "    # Transform Date into Year and JulianDay_Sin\n",
    "    dataset['Date'] = pd.to_datetime(dataset['Date'], utc=True)\n",
    "    dataset['Year'] = dataset['Date'].dt.year\n",
    "    dataset['JulianDay'] = dataset['Date'].dt.dayofyear\n",
    "    # Sine transformation to account for cyclical nature of Julian Day\n",
    "    dataset['JulianDay_Sin'] = np.sin(2*np.pi*dataset['JulianDay']/365) \n",
    "    dataset.drop(columns=['Date', 'JulianDay'], inplace=True)\n",
    "    \n",
    "    #Add year and JulianDay_Sin to oldCols \n",
    "    oldCols += ['Year', 'JulianDay_Sin']\n",
    "    \n",
    "    return dataset, oldCols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that will sort dataset into scaled X and Y\n",
    "# Pseudocode:\n",
    "# 1. Separate dataset into Features and Target\n",
    "# 2. Scale the Features array using MinMaxScaler\n",
    "# 3. Return the scaled X, Y, and the scaler used\n",
    "def scaleData(dataset):\n",
    "    # Separate Features and Target\n",
    "    features = dataset[FEATURES]\n",
    "    target = dataset[['O18', 'H2']]\n",
    "\n",
    "    # Scale the Features\n",
    "    scaler = MinMaxScaler()\n",
    "    X = scaler.fit_transform(features.values)\n",
    "    Y = target.values\n",
    "\n",
    "    return X, Y, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Builder Function \n",
    "# Pseudocode:\n",
    "# 1. Create a Sequential Model\n",
    "# 2. Add an Input Layer\n",
    "# 3. Prep the Search Space for Hyperparameter Tuning\n",
    "# 4. Add a LSTM Layer with Hyperparameters\n",
    "# 5. Add two Dense Layers with Hyperparameters \n",
    "# 6. Add a Dense Output Layer\n",
    "# 7. Compile the Model with Hyperparameters\n",
    "# 8. Return the Model\n",
    "def modelBuilder(numNeurons1, numNeurons2, numNeurons3, lr):\n",
    "    # Create a Sequential Model\n",
    "    model = Sequential()\n",
    "    # Add an Input Layer\n",
    "    model.add(InputLayer(shape=(len(FEATURES), 1)))\n",
    "\n",
    "    # Add the hidden layers with Hyperparameters\n",
    "    model.add(LSTM(numNeurons1))\n",
    "    model.add(Dense(numNeurons2, activation='relu'))\n",
    "    model.add(Dense(numNeurons3, activation='relu'))\n",
    "\n",
    "    # Add the Output Layer\n",
    "    model.add(Dense(2))\n",
    "\n",
    "    # Compile the Model with Hyperparameters\n",
    "    model.compile(optimizer=Adam(learning_rate=lr), loss='mse', metrics=[RootMeanSquaredError(), MeanAbsoluteError()])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter Search Space\n",
    "# Pseudocode:\n",
    "# 1. Define the search space for the hyperparameters\n",
    "# 2. Create a Hyperband Tuner Model from the search space\n",
    "# 3. Create a callback to stop training early\n",
    "# 4. Return the model\n",
    "def hyperParameterSearchSpace(hp):\n",
    "    # Prep the Search Space for Hyperparameter Tuning\n",
    "    hp_numNeurons1 = hp.Choice('numNeurons_LSTM', values=[2**3, 2**4, 2**5, 2**6, 2**7, 2**8, 2**9, 2**10])\n",
    "    hp_numNeurons2 = hp.Choice('numNeurons_Dense1', values=[2**3, 2**4, 2**5, 2**6, 2**7, 2**8, 2**9, 2**10])\n",
    "    hp_numNeurons3 = hp.Choice('numNeurons_Dense2', values=[2**3, 2**4, 2**5, 2**6, 2**7, 2**8, 2**9, 2**10])\n",
    "    hp_lr = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
    "\n",
    "    model = modelBuilder(hp_numNeurons1, hp_numNeurons2, hp_numNeurons3, hp_lr)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning process\n",
    "# Pseudocode:\n",
    "# 1. Create the Hyperband Tuner\n",
    "# 2. Create a callback to stop training early\n",
    "# 3. Perform the search\n",
    "# 4. Get the best model hyperparameters\n",
    "# 5. Return the best hyperparameters\n",
    "def hyperParameterTuning(xTrain, yTrain):\n",
    "    print('Start Tuning')\n",
    "    # Create the Hyperband Tuner\n",
    "    tuner = kt.Hyperband(hyperParameterSearchSpace, \n",
    "                        objective='val_loss', \n",
    "                        max_epochs=10, factor=3,\n",
    "                        directory='Hyperparameter_Tuning', project_name='Model_1',\n",
    "                        overwrite=True)\n",
    "\n",
    "    # Create a callback to stop training early\n",
    "    stop_early = EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "    # Perform the search\n",
    "    tuner.search(xTrain, yTrain, epochs=50, validation_split=0.2, callbacks=[stop_early], verbose=0)\n",
    "\n",
    "    # Get the best model hyperparameters\n",
    "    best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "    # Save the best hyperparameters to a JSON file\n",
    "    with open('Best_Hyperparameters.json', 'w') as f:\n",
    "        f.write(json.dumps(best_hps.values))\n",
    "\n",
    "    print('Finsihed Tuning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Process\n",
    "# Pseudocode:\n",
    "# 1. Load the best hyperparameters\n",
    "# 2. Using the best hyperparameters, build the model\n",
    "# 3. Early Stopping\n",
    "# 4. Train the model\n",
    "# 5. Return the trained model\n",
    "def trainModel(xTrain, yTrain):\n",
    "    print('Start Training')\n",
    "\n",
    "    # Load the best hyperparameters\n",
    "    with open('Best_Hyperparameters.json', 'r') as f:\n",
    "        hyperparams = json.load(f)\n",
    "\n",
    "    # Using the best hyperparameters, build the model\n",
    "    model = modelBuilder(hyperparams['numNeurons_LSTM'], hyperparams['numNeurons_Dense1'], hyperparams['numNeurons_Dense2'], hyperparams['learning_rate'])\n",
    "\n",
    "    # Early Stopping\n",
    "    stop_early = EarlyStopping(monitor='val_loss', patience = 100, restore_best_weights=True)\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(xTrain, yTrain, epochs=1000, validation_split=0.2, callbacks=[stop_early], verbose=0)\n",
    "\n",
    "    print('Finished Training')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test data prediction using the test data and the trained model\n",
    "# Pseudocode:\n",
    "# 1. Scale the test data using the scaler\n",
    "# 2. Predict the test data using the trained model\n",
    "# 3. Combine the test data and the predictions with original headers\n",
    "# 4. Save the results to a CSV\n",
    "def predictTestData(xTest, yTest, model, scaler, oldCols):\n",
    "    # Scale the test data using the scaler\n",
    "    x = scaler.transform(xTest.values)\n",
    "    \n",
    "    # Predict the test data using the trained model\n",
    "    yPreds = model.predict(x)\n",
    "\n",
    "    # Combine the test data and the predictions with original headers\n",
    "    testResults = pd.DataFrame(np.concatenate((xTest, yTest, yPreds), axis=1), columns=FEATURES + ['O18 A', 'H2 A', 'O18 P', 'H2 P'])\n",
    "\n",
    "    # Save the results to a CSV\n",
    "    testResults.to_csv(f'Model_{MODELNUM}_TestData.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Function\n",
    "def main():\n",
    "    print(f\"Model {MODELNUM} - {SCHEME}\")\n",
    "    print(\"---------------------------------\")\n",
    "\n",
    "    # Import train data and original headers\n",
    "    trainData, oldCols = importData('DataTrain')\n",
    "    print(\"Training Data Imported\")\n",
    "\n",
    "    # Scale and Split the train data\n",
    "    xTrain, yTrain, scaler = scaleData(trainData)\n",
    "    print(\"Training Data Scaled\")\n",
    "\n",
    "    # Hyperparameter Tuning\n",
    "    hyperParameterTuning(xTrain, yTrain)\n",
    "\n",
    "    # Train the Model\n",
    "    model = trainModel(xTrain, yTrain)\n",
    "\n",
    "    # Import test data and original headers\n",
    "    testData, _ = importData('DataTest')\n",
    "    print(\"Test Data Imported\")\n",
    "\n",
    "    # Predict the test data using the trained model\n",
    "    predictTestData(testData[FEATURES], testData[['O18', 'H2']], model, scaler, oldCols)\n",
    "    print(\"Test Data Predicted\")\n",
    "\n",
    "    # Save the model\n",
    "    model.save(f'Model_{MODELNUM}.keras')\n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
