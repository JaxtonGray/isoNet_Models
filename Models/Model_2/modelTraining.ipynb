{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Model ####\n",
    "MODELNUM = 2\n",
    "SCHEME = 'PrevailingWinds_6Split'\n",
    "FEATURES = ['Lat', 'Lon', 'Alt', 'Temp', 'Precip', 'Year', 'JulianDay_Sin']\n",
    "\n",
    "### Import Libraries\n",
    "# Base Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "# Tensorflow, scikit, kerasTuner\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, InputLayer\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import RootMeanSquaredError, MeanAbsoluteError\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import keras_tuner as kt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to import a dataset and transform headers for easier coding and convert Date column\n",
    "# Pseudocode:\n",
    "# 1. Import Dataset Save old Headers for later\n",
    "# 2. Transform headernames to rid of units\n",
    "# 3. Convert Date into Year and Julian Day\n",
    "# 4. Peform a Sine Transformation on JulianDay\n",
    "# 5. Return the dataset, and old headers\n",
    "def importData(fileName):\n",
    "    # Read in the correct file\n",
    "    dataset = pd.read_csv(f'../../Data/{fileName}.csv')\n",
    "    oldCols = list(dataset.columns)\n",
    "\n",
    "    # Remove any units (anything in parentheses)\n",
    "    codeCols = list(map(lambda x: re.sub(r'\\(([^()]*)\\)', '', x).strip(), oldCols))\n",
    "    dataset.columns = codeCols\n",
    "\n",
    "    # Transform Date into Year and JulianDay_Sin\n",
    "    dataset['Date'] = pd.to_datetime(dataset['Date'], utc=True)\n",
    "    dataset['Year'] = dataset['Date'].dt.year\n",
    "    dataset['JulianDay'] = dataset['Date'].dt.dayofyear\n",
    "    # Sine transformation to account for cyclical nature of Julian Day\n",
    "    dataset['JulianDay_Sin'] = np.sin(2*np.pi*dataset['JulianDay']/365) \n",
    "    dataset.drop(columns=['Date', 'JulianDay'], inplace=True)\n",
    "    \n",
    "    #Add year and JulianDay_Sin to oldCols \n",
    "    oldCols += ['Year', 'JulianDay_Sin']\n",
    "    \n",
    "    return dataset, oldCols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that will sort dataset into scaled X and Y\n",
    "# Pseudocode:\n",
    "# 1. Separate dataset into Features and Target\n",
    "# 2. Scale the Features array using MinMaxScaler\n",
    "# 3. Return the scaled X, Y, and the scaler used\n",
    "def scaleData(dataset, regionalScaler = None):\n",
    "    # Separate Features and Target\n",
    "    features = dataset[FEATURES]\n",
    "    target = dataset[['O18', 'H2']]\n",
    "\n",
    "    # Scale the data if no regionalScaler is provided\n",
    "    if regionalScaler is None:\n",
    "        # Scale the Features\n",
    "        scaler = MinMaxScaler()\n",
    "        X = scaler.fit_transform(features.values)\n",
    "        Y = target.values\n",
    "        return X, Y, scaler\n",
    "    \n",
    "    else:\n",
    "        # Scale the Features using the regionalScaler provided\n",
    "        X = regionalScaler.transform(features.values)\n",
    "        Y = target.values\n",
    "        scaler = regionalScaler\n",
    "        return X, Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data based on spatial scheme\n",
    "# Pseudocode:\n",
    "# 1. Load in the schematic file\n",
    "# 2. Convert the schematic file into a geodataframe\n",
    "# 3. Convert the dataset into a geodataframe\n",
    "# 4. Split data based on the spatial scheme region and add to a dictionary\n",
    "# 5. Return the dictionary of dataframes\n",
    "def schemeSplit(df):\n",
    "    # Load in the schematic file\n",
    "    scheme = pd.read_csv(f'../../Data/ModelSplit_Schemes/{SCHEME}.csv')\n",
    "    # Convert the schematic file into a geodataframe\n",
    "    scheme = gpd.GeoDataFrame(scheme, geometry=gpd.GeoSeries.from_wkt(scheme['geometry']))\n",
    "\n",
    "    # Convert the dataset into a geodataframe\n",
    "    gdf = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df.Lon, df.Lat))\n",
    "\n",
    "    # Split data based on the spatial scheme region and add to a dictionary\n",
    "    splitData = {}\n",
    "\n",
    "    for region in scheme.iterrows():\n",
    "        regionData = gdf[gdf.within(region[1]['geometry'])].reset_index()\n",
    "        regionData_X, regionData_Y, regionData_scaler = scaleData(regionData)\n",
    "        splitData[region[1]['Region']] = (regionData_X, regionData_Y, regionData_scaler)\n",
    "\n",
    "    return splitData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Builder Function \n",
    "# Pseudocode:\n",
    "# 1. Create a Sequential Model\n",
    "# 2. Add an Input Layer\n",
    "# 3. Prep the Search Space for Hyperparameter Tuning\n",
    "# 4. Add a LSTM Layer with Hyperparameters\n",
    "# 5. Add two Dense Layers with Hyperparameters \n",
    "# 6. Add a Dense Output Layer\n",
    "# 7. Compile the Model with Hyperparameters\n",
    "# 8. Return the Model\n",
    "def modelBuilder(numNeurons1, numNeurons2, numNeurons3, lr):\n",
    "    # Create a Sequential Model\n",
    "    model = Sequential()\n",
    "    # Add an Input Layer\n",
    "    model.add(InputLayer(shape=(len(FEATURES), 1)))\n",
    "\n",
    "    # Add the hidden layers with Hyperparameters\n",
    "    model.add(LSTM(numNeurons1))\n",
    "    model.add(Dense(numNeurons2, activation='relu'))\n",
    "    model.add(Dense(numNeurons3, activation='relu'))\n",
    "\n",
    "    # Add the Output Layer\n",
    "    model.add(Dense(2))\n",
    "\n",
    "    # Compile the Model with Hyperparameters\n",
    "    model.compile(optimizer=Adam(learning_rate=lr), loss='mse', metrics=[RootMeanSquaredError(), MeanAbsoluteError()])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter Search Space\n",
    "# Pseudocode:\n",
    "# 1. Define the search space for the hyperparameters\n",
    "# 2. Create a Hyperband Tuner Model from the search space\n",
    "# 3. Create a callback to stop training early\n",
    "# 4. Return the model\n",
    "def hyperParameterSearchSpace(hp):\n",
    "    # Prep the Search Space for Hyperparameter Tuning\n",
    "    hp_numNeurons1 = hp.Choice('numNeurons_LSTM', values=[2**3, 2**4, 2**5, 2**6, 2**7, 2**8, 2**9, 2**10])\n",
    "    hp_numNeurons2 = hp.Choice('numNeurons_Dense1', values=[2**3, 2**4, 2**5, 2**6, 2**7, 2**8, 2**9, 2**10])\n",
    "    hp_numNeurons3 = hp.Choice('numNeurons_Dense2', values=[2**3, 2**4, 2**5, 2**6, 2**7, 2**8, 2**9, 2**10])\n",
    "    hp_lr = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
    "\n",
    "    model = modelBuilder(hp_numNeurons1, hp_numNeurons2, hp_numNeurons3, hp_lr)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning process\n",
    "# Pseudocode:\n",
    "# 1. Create the Hyperband Tuner\n",
    "# 2. Create a callback to stop training early\n",
    "# 3. Perform the search\n",
    "# 4. Get the best model hyperparameters\n",
    "# 5. Return the best hyperparameters\n",
    "def hyperParameterTuning(xTrain, yTrain):\n",
    "    print('Start Tuning')\n",
    "    # Create the Hyperband Tuner\n",
    "    tuner = kt.Hyperband(hyperParameterSearchSpace, \n",
    "                        objective='val_loss', \n",
    "                        max_epochs=10, factor=3,\n",
    "                        directory='Hyperparameter_Tuning', project_name='Model_1',\n",
    "                        overwrite=True)\n",
    "\n",
    "    # Create a callback to stop training early\n",
    "    stop_early = EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "    # Perform the search\n",
    "    tuner.search(xTrain, yTrain, epochs=50, validation_split=0.2, callbacks=[stop_early], verbose=0)\n",
    "\n",
    "    # Get the best model hyperparameters\n",
    "    best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "    print('Finsihed Tuning')\n",
    "    # Return the best hyperparameters\n",
    "    return best_hps.values\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Process\n",
    "# Pseudocode:\n",
    "# 1. Load the best hyperparameters\n",
    "# 2. Using the best hyperparameters, build the model\n",
    "# 3. Early Stopping\n",
    "# 4. Train the model\n",
    "# 5. Return the trained model\n",
    "def trainModel(xTrain, yTrain, hyperparams):\n",
    "    print('Start Training')\n",
    "\n",
    "    # Using the best hyperparameters, build the model\n",
    "    model = modelBuilder(hyperparams['numNeurons_LSTM'], hyperparams['numNeurons_Dense1'], hyperparams['numNeurons_Dense2'], hyperparams['learning_rate'])\n",
    "\n",
    "    # Early Stopping\n",
    "    stop_early = EarlyStopping(monitor='val_loss', patience = 100, restore_best_weights=True)\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(xTrain, yTrain, epochs=1000, validation_split=0.2, callbacks=[stop_early], verbose=0)\n",
    "\n",
    "    print('Finished Training')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and tune all models for non-global schemes\n",
    "# Pseudocode:\n",
    "# 1. Cycle through all the regional datasets\n",
    "# 2. Tune the model for each region\n",
    "# 3. Save the best hyperparameters to a new region dictionary\n",
    "# 4. Train the model for each region and save the trained model to the regional model dictionary\n",
    "# 5. Return the regional model dictionary\n",
    "def traintuneAllModels(regionalData):\n",
    "    print(\"Start tuning and training all models\") \n",
    "    # Cycle through all the regional datasets\n",
    "    regionalModels = {}\n",
    "    regionalHyperparams = {}\n",
    "\n",
    "    for region in regionalData.keys():\n",
    "        print(f'-----> Region: {region}')\n",
    "        # Tune the model for each region\n",
    "        bestHyperparams = hyperParameterTuning(regionalData[region][0], regionalData[region][1])\n",
    "        regionalHyperparams[region] = bestHyperparams\n",
    "\n",
    "        # Train the model for each region\n",
    "        model = trainModel(regionalData[region][0], regionalData[region][1], bestHyperparams)\n",
    "        regionalModels[region] = model\n",
    "\n",
    "    # Save the regional model dictionary to a file\n",
    "    with open(f'../../Models/Model_{MODELNUM}_{SCHEME}_Models.json', 'w') as f:\n",
    "        json.dump(regionalModels, f)\n",
    "\n",
    "    return regionalModels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test data prediction using the test data and the trained model\n",
    "# Pseudocode:\n",
    "# 1. Scale the test data using the scaler\n",
    "# 2. Predict the test data using the trained model\n",
    "# 3. Combine the test data and the predictions with original headers\n",
    "# 4. Save the results to a CSV\n",
    "def predictTestData(xTest, yTest, model, scaler):\n",
    "    # Scale the test data using the scaler\n",
    "    x = scaler.transform(xTest.values)\n",
    "    \n",
    "    # Predict the test data using the trained model\n",
    "    yPreds = model.predict(x)\n",
    "\n",
    "    # Combine the test data and the predictions with original headers\n",
    "    testResults = pd.DataFrame(np.concatenate((xTest, yTest, yPreds), axis=1), columns=FEATURES + ['O18 A', 'H2 A', 'O18 P', 'H2 P'])\n",
    "\n",
    "    # Save the results to a CSV\n",
    "    testResults.to_csv(f'Model_{MODELNUM}_TestData.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict all test data for all regional models for non-global schemes\n",
    "# Pseudocode:\n",
    "# 1. Load in test data and scheme\n",
    "# 2. Cycle through all the regions\n",
    "# 3. Scale the test data using the scaler for each region\n",
    "# 4. Predict the test data using the trained model for each region\n",
    "# 5. Combine the test data and the predictions with original headers for each region\n",
    "# 6. Save the results to a CSV for each region\n",
    "def predictAllTestData(regionalModels, regionalData):\n",
    "    print(\"Predicting for all test data\")\n",
    "    testData = pd.read_csv('../../Data/TestData.csv')[0]\n",
    "    # Convert testData into geoDataFrame\n",
    "    gdf = gpd.GeoDataFrame(testData, geometry=gpd.points_from_xy(testData.Lon, testData.Lat))\n",
    "\n",
    "    # Load in the schematic file\n",
    "    scheme = pd.read_csv(f'../../Data/ModelSplit_Schemes/{SCHEME}.csv')\n",
    "    # Convert the schematic file into a geodataframe\n",
    "    scheme = gpd.GeoDataFrame(scheme, geometry=gpd.GeoSeries.from_wkt(scheme['geometry']))\n",
    "\n",
    "    regionalPredictions = pd.DataFrame(columns=FEATURES + ['O18 A', 'H2 A', 'O18 P', 'H2 P'])\n",
    "    for region in scheme.iterrows():\n",
    "        # Grab the regional model and the scaler for the region\n",
    "        model = regionalModels[region[1]['Region']]\n",
    "        scaler = regionalData[region[1]['Region']][2]\n",
    "        testData = gdf[gdf.within(region[1]['geometry'])].reset_index()\n",
    "\n",
    "        # Scale the test data using the scaler for each region\n",
    "        xTest = scaler.transform(testData[FEATURES].values)\n",
    "        yTest = testData[['O18', 'H2']].values\n",
    "\n",
    "        # Predict the test data using the trained model for each region\n",
    "        yPreds = model.predict(xTest)\n",
    "\n",
    "        # Combine the test data and the predictions with original headers for each region\n",
    "        testResults = pd.DataFrame(np.concatenate((xTest, yTest, yPreds), axis=1), \n",
    "                                   columns=FEATURES + ['O18 A', 'H2 A', 'O18 P', 'H2 P'])\n",
    "        \n",
    "        regionalPredictions = pd.concat([regionalPredictions, testResults], axis=0)\n",
    "\n",
    "        if not os.path.exists(f'Trained_Models'):\n",
    "            os.makedirs(f'Trained_Models')\n",
    "            # Save the model to a file\n",
    "            model.save(f'Trained_Models/{region[1][\"Region\"]}.keras')\n",
    "        else:\n",
    "            # Save the model to a file\n",
    "            model.save(f'Trained_Models/{region[1][\"Region\"]}.keras')\n",
    "\n",
    "        \n",
    "\n",
    "    # Save all the results to a CSV\n",
    "    regionalPredictions.to_csv(f'Model_{MODELNUM}_TestData.csv', index=False)\n",
    "\n",
    "\n",
    "    print(\"Finished predicting all test data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 2 - PrevailingWinds_6Split\n",
      "---------------------------------\n",
      "Training Data Imported\n",
      "\n",
      "Splitting training data based on Scheme: PrevailingWinds_6Split\n",
      "Start tuning and training all models\n",
      "-----> Region: PolarWesterlies_NW\n",
      "Start Tuning\n"
     ]
    }
   ],
   "source": [
    "# Main Function\n",
    "def main():\n",
    "    print(f\"Model {MODELNUM} - {SCHEME}\")\n",
    "    print(\"---------------------------------\")\n",
    "\n",
    "    # Import train data and original headers\n",
    "    trainData, oldCols = importData('DataTrain')\n",
    "    print(\"Training Data Imported\")\n",
    "\n",
    "    # If a global spatial scheme is used do not split the data\n",
    "    if SCHEME == \"Global\":\n",
    "        # Scale and Split the train data\n",
    "        xTrain, yTrain, scaler = scaleData(trainData)\n",
    "        print(\"Training Data Scaled\")\n",
    "\n",
    "        # Hyperparameter Tuning\n",
    "        best_hps = hyperParameterTuning(xTrain, yTrain)\n",
    "\n",
    "        # Train the Model\n",
    "        model = trainModel(xTrain, yTrain, best_hps)\n",
    "\n",
    "        # Import test data and original headers\n",
    "        testData = importData('DataTest')[0]\n",
    "        print(\"Test Data Imported\")\n",
    "\n",
    "        # Predict the test data using the trained model\n",
    "        predictTestData(testData[FEATURES], testData[['O18', 'H2']], model, scaler)\n",
    "        print(\"Test Data Predicted\")\n",
    "\n",
    "        # Save the model\n",
    "        model.save(f'Model_{MODELNUM}.keras')\n",
    "    else:\n",
    "        # Split the data based on the spatial scheme\n",
    "        print(f\"\\nSplitting training data based on Scheme: {SCHEME}\")\n",
    "        splitData = schemeSplit(trainData)\n",
    "        \n",
    "        # Train and tune all models for non-global schemes\n",
    "        regionalModels = traintuneAllModels(splitData)\n",
    "\n",
    "        # Predict all test data for all regional models for non-global schemes\n",
    "        predictAllTestData(regionalModels, splitData)\n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
