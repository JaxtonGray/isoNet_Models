{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the common libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import re\n",
    "\n",
    "# TensorFlow and Scikit Learn libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, InputLayer\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import RootMeanSquaredError, MeanAbsoluteError\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function that imports the data and returns 6 dataframes for each of the 6 models\n",
    "def importData():\n",
    "    # Importing the data and convert to a GeoDF\n",
    "    df = pd.read_csv(r'../../Data/GNIP/GNIP_Cleaned.csv')\n",
    "    df['Date'] = pd.to_datetime(df['Date'], utc=True)\n",
    "    df = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df.Lon, df.Lat, df.Alt)).set_crs('EPSG:4326')\n",
    "    cols = df.columns\n",
    "\n",
    "    # Remove units from columns for easier processing\n",
    "    codeCols = list(map(lambda x: re.sub(r'\\(([^()]*)\\)', '', x).strip(), cols))\n",
    "    df.columns = codeCols\n",
    "\n",
    "    # Load in the PrevailingWinds file for which the model will be split on\n",
    "    modelLocations = pd.read_csv(r'../../Data/ModelSplit_Arch/PrevailingWinds_6Split.csv')\n",
    "    modelLocations = gpd.GeoDataFrame(modelLocations, geometry=gpd.GeoSeries.from_wkt(modelLocations['Geometry']), crs=\"EPSG:4326\")\n",
    "    modelLocations.drop(columns=\"Geometry\", inplace=True)\n",
    "    modelLocations.set_index('Prevailing Wind', inplace=True)\n",
    "\n",
    "    # Create an empty dictionary that will contain each of the modelDatasets to train with\n",
    "    modelData = {}\n",
    "    for modelLoc in modelLocations.index:\n",
    "        # Create a new dataframe for each of the model locations and store it in the dictionary\n",
    "        modelData[modelLoc] = df[df.within(modelLocations.loc[modelLoc].geometry)].copy()\n",
    "        modelData[modelLoc] = pd.DataFrame(modelData[modelLoc])\n",
    "        # Drop the geometry column as it is not needed for the model\n",
    "        modelData[modelLoc].drop(columns='geometry', inplace=True)\n",
    "\n",
    "    return df, cols, modelData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will have a function that will setup the data for the model\n",
    "def dataSetup(modelData, modelName):\n",
    "    # Create the features and target variables\n",
    "    dataset = modelData[modelName]\n",
    "    targets = dataset[['O18', 'H2']]\n",
    "    features = dataset.drop(columns=['O18', 'H2'])\n",
    "\n",
    "    # Extract the year and julian day from the date, convert to sin transformation for julian day\n",
    "    features['Date'] = pd.to_datetime(features['Date'], utc=True)\n",
    "    features['Year'] = features['Date'].dt.year\n",
    "    features['JulianDay'] = features['Date'].dt.dayofyear\n",
    "\n",
    "    # Create the sin transformation for the julian day\n",
    "    features['JulianDay_Sin'] = np.sin(2 * np.pi * features['JulianDay'] / 365)\n",
    "    features.drop(columns=['Date', 'JulianDay'], inplace=True)\n",
    "    trainingCols = features.columns\n",
    "\n",
    "    # Create the Scaler object and fit the features\n",
    "    scaler = MinMaxScaler()\n",
    "    X = scaler.fit_transform(features.values)\n",
    "    y = targets.values\n",
    "\n",
    "    # Split the data into training and testing\n",
    "    xTrain, xTest, yTrain, yTest = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Split the training data into training and validation\n",
    "    xVal, yVal = xTrain[:int(len(xTrain)*0.2)], yTrain[:int(len(yTrain)*0.2)]\n",
    "    xTrain, yTrain = xTrain[int(len(xTrain)*0.2):], yTrain[int(len(yTrain)*0.2):]\n",
    "\n",
    "    return xTrain, xVal, xTest, yTrain, yVal, yTest, scaler, trainingCols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function that will create the model\n",
    "def create_model(neurons, lr, numFeatures):\n",
    "    model = Sequential()\n",
    "    model.add(InputLayer(shape=(numFeatures,1)))\n",
    "    model.add(LSTM(neurons))\n",
    "    model.add(Dense(neurons))\n",
    "    model.add(Dense(neurons))\n",
    "    model.add(Dense(2)) # 2 outputs\n",
    "    model.compile(optimizer=Adam(learning_rate=lr), loss='mse', metrics=[RootMeanSquaredError(), MeanAbsoluteError()])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will train a model\n",
    "def modelTrain(model, xTrain, yTrain, xVal, yVal, epochs):\n",
    "    earlyStop = EarlyStopping(monitor='val_loss', mode='min', patience=10, verbose=1)\n",
    "    model.fit(xTrain, yTrain, epochs=epochs, validation_data=(xVal, yVal), callbacks=[earlyStop], verbose=0)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will go through the process of training each of the models\n",
    "def trainAllModels(modelData):\n",
    "    # Cycle through each of the models and create an empty dictionary to store the models\n",
    "    models = {}\n",
    "    testData = {}\n",
    "    for modelName in modelData.keys():\n",
    "        # Setup the data for the model\n",
    "        xTrain, xVal, xTest, yTrain, yVal, yTest, scaler = dataSetup(modelData, modelName)\n",
    "        numFeatures = xTrain.shape[1]\n",
    "\n",
    "        # Create the model\n",
    "        model = create_model(64, 0.001, numFeatures)\n",
    "        model = modelTrain(model, xTrain, yTrain, xVal, yVal, 500)\n",
    "\n",
    "        # Store the model in the dictionary\n",
    "        models[modelName] = model\n",
    "\n",
    "        # Store the test data in the dictionary\n",
    "        testData[modelName] = [xTest, yTest, scaler]\n",
    "\n",
    "    return models, testData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will predict the values for the test data based on geography\n",
    "def predictValues(models, testData, cols):\n",
    "    # Create an empty dictionary to store the predictions\n",
    "    predictions = {}\n",
    "    for modelName in models.keys():\n",
    "        # Extract the test data\n",
    "        xTest, yTest, scaler = testData[modelName]\n",
    "\n",
    "        # Predict the values\n",
    "        yPred = models[modelName].predict(xTest)\n",
    "\n",
    "        # Inverse the scaling\n",
    "        xTest = scaler.inverse_transform(xTest)\n",
    "\n",
    "        # Store the predictions\n",
    "        predictions[modelName] = [xTest, yPred, yTest]\n",
    "    \n",
    "    # Combine the predictions into a single dataframe\n",
    "    predDF = pd.DataFrame(columns=cols)\n",
    "    for modelName in predictions.keys():\n",
    "        xTest, yPred, yTest = predictions[modelName]\n",
    "        tempDF = pd.DataFrame(xTest, columns=cols[:-2])\n",
    "        tempDF['O18'] = yPred[:,0]\n",
    "        tempDF['H2'] = yPred[:,1]\n",
    "        predDF = pd.concat([predDF, tempDF], ignore_index=True)\n",
    "\n",
    "    return predDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export all information to file\n",
    "def exportData(models, predDF):\n",
    "    # Export the models\n",
    "    for modelName in models.keys():\n",
    "        models[modelName].save(f'Trained_Models/{modelName}.keras')\n",
    "    \n",
    "    # Export the predictions\n",
    "    predDF.to_csv('Model_2_TestData.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function to call all the other functions\n",
    "def main():\n",
    "    # Importing the data\n",
    "    df, cols, modelData = importData()\n",
    "    models, testData = trainAllModels(modelData)\n",
    "    predictions = predictValues(models, testData, cols)\n",
    "    exportData(models, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = dataSetup(importData()[2], 'Trade_W')[-1]\n",
    "cols = list(cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Lat',\n",
       " 'Lon',\n",
       " 'Alt',\n",
       " 'Precip',\n",
       " 'Temp',\n",
       " 'Year',\n",
       " 'JulianDay_Sin',\n",
       " 'O18A',\n",
       " 'H2A',\n",
       " 'O18P',\n",
       " 'H2P']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = cols + ['O18A', 'H2A', 'O18P', 'H2P']\n",
    "cols"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
