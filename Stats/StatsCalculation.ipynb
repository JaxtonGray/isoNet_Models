{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################################################\n",
    "#  This will read in a model number and output the stats for that specific model.\n",
    "# It will calculate the KGE (Kling-Gupta Efficiency), RMSE, and Residuals for the model.\n",
    "# However, given that some of these models have different regional models that make up the main model,\n",
    "# it will need to be separated into different regions. But only for the models that do have regional models.\n",
    "# The stats will be outputted to a csv file. To be used in the future for analysis.\n",
    "####################################################################################################\n",
    "\n",
    "import json\n",
    "from glob import glob\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caclulate the RMSE for a predicted and observed dataset\n",
    "# Assume that the observed and predicted datasets are pandas dataframes with the same length\n",
    "def rmse(observed, predicted):\n",
    "    return np.sqrt(np.mean((observed - predicted) ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the KGE for a predicted and observed dataset\n",
    "# Assume that the observed and predicted datasets are pandas dataframes with the same length\n",
    "def kge(observed, predicted):\n",
    "    # Calculate mean and standard deviation of observed and predicted datasets\n",
    "    obs_mean = observed.mean()\n",
    "    sim_mean = predicted.mean()\n",
    "    obs_std = observed.std()\n",
    "    sim_std = predicted.std()\n",
    "\n",
    "    # Calculate the correlation coefficient between the observed and predicted datasets\n",
    "    r = np.corrcoef(observed, predicted)[0, 1] # [0, 1] is the correlation between the two datasets in the matrix\n",
    "\n",
    "    # Calculate the bias and variability between the observed and predicted datasets\n",
    "    beta = sim_mean / obs_mean\n",
    "    alpha = sim_std / obs_std\n",
    "\n",
    "    # Calculate the Kling-Gupta Efficiency\n",
    "    return 1 - np.sqrt((r - 1) ** 2 + (alpha - 1) ** 2 + (beta - 1) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function that will load in the model data and return a geopandas dataframe with the data and\n",
    "# the model architecture\n",
    "def readModelData(modelNum):\n",
    "    # Load in the model data\n",
    "    modelData = pd.read_csv(f\"../Models/Model_{modelNum}/Model_{modelNum}_TestData.csv\")\n",
    "    modelData = gpd.GeoDataFrame(modelData, geometry=gpd.points_from_xy(modelData.Lon, modelData.Lat), crs=\"EPSG:4326\")\n",
    "\n",
    "    # Change the headers of the columns to be more usable with the code\n",
    "    # Start by extracting the old headers\n",
    "    oldHeaders = modelData.columns\n",
    "\n",
    "    # Now go through and change the headers to be more usable using regex to remove anything within brackets\n",
    "    # Also if there is something like O18 Actual, it will be changed to O18A and O18P for the predicted\n",
    "    newHeaders = []\n",
    "    newHeaders = [re.sub(r\"\\(.*\\)\", \"\", header).strip() for header in oldHeaders]\n",
    "    newHeaders = [re.sub(r\" Actual\", \"A\", header).strip() for header in newHeaders]\n",
    "    newHeaders = [re.sub(r\" Predicted\", \"P\", header).strip() for header in newHeaders]\n",
    "\n",
    "    # Now change the headers of the dataframe\n",
    "    modelData.columns = newHeaders\n",
    "    \n",
    "\n",
    "    return modelData, list(oldHeaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readModelArch(modelArch_Name):\n",
    "    # Load in the model architecture which will only need to be done if not \"Global\"\n",
    "    if modelArch_Name != \"Global\":\n",
    "        modelArch = pd.read_csv(f\"../Data/ModelSplit_Arch/{modelArch_Name}.csv\")\n",
    "        modelArch = gpd.GeoDataFrame(modelArch, geometry=gpd.GeoSeries.from_wkt(modelArch.geometry), crs=\"EPSG:4326\")\n",
    "    else:\n",
    "        modelArch = None\n",
    "    \n",
    "    return modelArch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the summary statistics for the model and return a dataframe with the results for each region\n",
    "# and if there are no regions, return the stats for the entire model.\n",
    "def summaryStats(modelData, modelArch_name):\n",
    "    # First check if the model has regional models\n",
    "    if modelArch_name != \"Global\":\n",
    "        # Load in the model architecture\n",
    "        modelArch = readModelArch(modelArch_name)\n",
    "\n",
    "        # Cycle through the regions and assign the data from each to dictionary\n",
    "        regionalData = {}\n",
    "        key = modelArch.columns[0]\n",
    "        for null, region in modelArch.iterrows():\n",
    "            regionalData[region[key]] = modelData[modelData.within(region.geometry)]\n",
    "        \n",
    "        # Calculate the stats for each region and store in a dictionary\n",
    "        regionalStats = pd.DataFrame(columns=[\"Region\", \"O18 KGE\", \"O18 RMSE\", \"H2 KGE\", \"H2 RMSE\"])\n",
    "        for region, data in regionalData.items():\n",
    "            stats = {\"Region\": region,\n",
    "                     \"O18 KGE\": kge(data['O18 A'], data['O18 P']),\n",
    "                     \"O18 RMSE\": rmse(data['O18 A'], data['O18 P']),\n",
    "                     \"H2 KGE\": kge(data['H2 A'], data['H2 P']),\n",
    "                     \"H2 RMSE\": rmse(data['H2 A'], data['H2 P']),}\n",
    "            statsDF = pd.DataFrame([stats], index=[0])\n",
    "            regionalStats = pd.concat([regionalStats, statsDF], ignore_index=True)\n",
    "        \n",
    "        return regionalStats\n",
    "    else:\n",
    "        # Calculate the stats for the entire model\n",
    "        stats = {\"O18 KGE\": kge(modelData['O18 A'], modelData['O18 P']),\n",
    "                 \"O18 RMSE\": rmse(modelData['O18 A'], modelData['O18 P']),\n",
    "                 \"H2 KGE\": kge(modelData['H2 A'], modelData['H2 P']),\n",
    "                 \"H2 RMSE\": rmse(modelData['H2 A'], modelData['H2 P']),}\n",
    "        \n",
    "        return pd.DataFrame([stats])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will cylce through all model architectures and calculate the stats for each model and return\n",
    "# a dictionary with the results: {modelArch_Name: stats, modelArch_Name2: stats2, ...}\n",
    "def allArchStats(modelData, mainArch):\n",
    "    # Use glob to get all the model architecture names\n",
    "    allArchs = glob(\"../Data/ModelSplit_Arch/*.csv\")\n",
    "    allArchs = [arch.split(\"/\")[-1].split(\".\")[0] for arch in allArchs]\n",
    "    allArchs = [re.split(r\"\\\\|//\", arch)[1] for arch in allArchs]\n",
    "    allArchs.append(\"Global\")\n",
    "    # Create a dictionary to store the stats for each model architecture\n",
    "    allArchsStats = {}\n",
    "\n",
    "    # Cycle through each model architecture and calculate the stats for each model\n",
    "    for arch in allArchs:\n",
    "        if arch == mainArch:\n",
    "            allArchsStats[f\"{arch} (main)\"] = summaryStats(modelData, arch)\n",
    "        else:\n",
    "            allArchsStats[arch] = summaryStats(modelData, arch)\n",
    "\n",
    "    return allArchsStats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the stats to an excel file where each sheet is a different model architecture\n",
    "def exportStats(allsStats, modelNum):\n",
    "    with pd.ExcelWriter(f\"SummaryStats//Model_{modelNum}_Stats.xlsx\") as writer:\n",
    "        for arch, stats in allsStats.items():\n",
    "            stats.to_excel(writer, sheet_name=arch, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Julian Day Sin to Julian Day\n",
    "def undoJulianDaySin(values):\n",
    "    return np.ceil((np.arcsin(values) / np.pi + 0.5) * 365).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caclulate the residuals and export them to a csv file\n",
    "def calculateResiduals(modelData, modelNum, oldHeaders):\n",
    "    # Calculate the residuals for the model\n",
    "    modelData['O18 Residuals'] = modelData['O18 A'] - modelData['O18 P']\n",
    "    modelData['H2 Residuals'] = modelData['H2 A'] - modelData['H2 P']\n",
    "\n",
    "    # Create a dictionary that will rename the columns back to the original names\n",
    "    renameDict = dict(zip(modelData.columns, oldHeaders))\n",
    "    modelData.rename(columns=renameDict, inplace=True)\n",
    "    modelData.drop(columns=['geometry'], inplace=True)\n",
    "\n",
    "    # Convert the Julian Day sine transform back to the original Julian Date\n",
    "    modelData['JulianDay'] = undoJulianDaySin(modelData['JulianDaySin'])\n",
    "\n",
    "    # Combine the Year and Julian Day columns to create a date column\n",
    "    modelData['Date'] = modelData['Year'].astype(int).astype(str) + \"-\" + modelData['JulianDay'].astype(str)\n",
    "    modelData['Date'] = pd.to_datetime(modelData['Date'], format=\"%Y-%j\")\n",
    "    modelData.drop(columns=['Year', 'JulianDay', 'JulianDaySin'], inplace=True)\n",
    "\n",
    "    # Move the Date column to the front of the dataframe\n",
    "    cols = modelData.columns.tolist()\n",
    "    cols = cols[-1:] + cols[:-1]\n",
    "    modelData = modelData[cols]\n",
    "    \n",
    "\n",
    "    # Export the residuals to a csv file\n",
    "    modelData.to_csv(f\"Residuals//Model_{modelNum}_Residuals.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Load in the model directory\n",
    "    with open(r'../Models/modelDirectory.json', 'r') as file:\n",
    "        data = json.load(file)\n",
    "    \n",
    "    # Cycle through the models and calculate the stats for each model\n",
    "    for modelValues in data.values():\n",
    "        modelNum = modelValues['num']\n",
    "        mainArch = modelValues['arch']\n",
    "\n",
    "        # Load in the model data\n",
    "        modelData, oldHeaders = readModelData(modelNum)\n",
    "\n",
    "        # Calculate the stats for the model and export to an excel file\n",
    "        statsAll = allArchStats(modelData, mainArch)\n",
    "        exportStats(statsAll, modelNum)\n",
    "        calculateResiduals(modelData, modelNum, oldHeaders)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
